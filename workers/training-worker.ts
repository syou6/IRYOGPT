/**
 * å­¦ç¿’ã‚¸ãƒ§ãƒ–ãƒ¯ãƒ¼ã‚«ãƒ¼
 * 
 * Redisã‚­ãƒ¥ãƒ¼ã‹ã‚‰å­¦ç¿’ã‚¸ãƒ§ãƒ–ã‚’å–å¾—ã—ã¦å‡¦ç†ã™ã‚‹
 * æ—¢å­˜ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’å†åˆ©ç”¨
 */

import { Worker, Job } from 'bullmq';
import Redis from 'ioredis';
import { createClient } from '@supabase/supabase-js';
import { Document } from '@langchain/core/documents';
import { CustomWebLoader } from '../utils/custom_web_loader';
import { OpenAIEmbeddings } from '@langchain/openai';
// @ts-ignore - LangChain 1.x module resolution issue
import { SupabaseVectorStore } from '@langchain/community/vectorstores/supabase';
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';

const MAX_TRAINING_PAGES = 60;

// Redisæ¥ç¶š
// Upstash Redisã¯TLSæ¥ç¶šãŒå¿…è¦
const isUpstash = process.env.REDIS_HOST?.includes('upstash.io');
const connection = new Redis({
  host: process.env.REDIS_HOST || 'localhost',
  port: parseInt(process.env.REDIS_PORT || '6379'),
  password: process.env.REDIS_PASSWORD,
  maxRetriesPerRequest: null,
  enableReadyCheck: false,
  ...(isUpstash && {
    tls: {
      rejectUnauthorized: false, // Upstashã®è¨¼æ˜æ›¸ã‚’ä¿¡é ¼
    },
  }),
});

// Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆã‚µãƒ¼ãƒ“ã‚¹ãƒ­ãƒ¼ãƒ«ã‚­ãƒ¼ä½¿ç”¨ï¼‰
const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

// ã‚¸ãƒ§ãƒ–ãƒ‡ãƒ¼ã‚¿ã®å‹å®šç¾©
interface TrainingJobData {
  site_id: string;
  baseUrl: string;
  sitemapUrl?: string;
  urlList?: string[];
  ownerUserId: string;
  requestedBy?: string;
  forceRetrain?: boolean;
}

// Sitemapã‹ã‚‰URLãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾å¿œï¼‰
async function getUrlsFromSitemap(sitemapUrl: string): Promise<string[]> {
  try {
    const response = await fetch(sitemapUrl);
    const xml = await response.text();
    
    // ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆsitemapindex.xmlï¼‰ã‹ã©ã†ã‹ã‚’ç¢ºèª
    if (xml.includes('<sitemapindex')) {
      // ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å ´åˆã€å„ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®URLã‚’å–å¾—
      const sitemapMatches = xml.match(/<sitemap>[\s\S]*?<loc>(.*?)<\/loc>[\s\S]*?<\/sitemap>/g);
      if (!sitemapMatches) return [];
      
      const sitemapUrls = sitemapMatches.map((match) => {
        const locMatch = match.match(/<loc>(.*?)<\/loc>/);
        if (!locMatch) return null;
        let url = locMatch[1];
        // CDATAã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†
        url = url.replace(/<!\[CDATA\[(.*?)\]\]>/g, '$1');
        return url.trim();
      }).filter((url): url is string => url !== null && url.length > 0);
      
      // å„ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‹ã‚‰URLã‚’å–å¾—ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
      const urlPromises = sitemapUrls.map((url) => getUrlsFromSitemap(url));
      const urlArrays = await Promise.all(urlPromises);
      
      // é‡è¤‡ã‚’é™¤å»ã—ã¦è¿”ã™
      const allUrls = urlArrays.flat();
      return Array.from(new Set(allUrls));
    } else {
      // é€šå¸¸ã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å ´åˆ
      const urlMatches = xml.match(/<url>[\s\S]*?<loc>(.*?)<\/loc>[\s\S]*?<\/url>/g);
      if (!urlMatches) {
        // <url>ã‚¿ã‚°ãŒãªã„å ´åˆã€<loc>ã‚¿ã‚°ã‚’ç›´æ¥æ¤œç´¢ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        const simpleMatches = xml.match(/<loc>(.*?)<\/loc>/g);
        if (!simpleMatches) return [];
        return simpleMatches
          .map((match) => {
            const url = match.replace(/<\/?loc>/g, '');
            // CDATAã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†
            return url.replace(/<!\[CDATA\[(.*?)\]\]>/g, '$1').trim();
          })
          .filter((url) => url.length > 0);
      }
      
      // <url>ã‚¿ã‚°å†…ã®<loc>ã‚’æŠ½å‡º
      const urls = urlMatches.map((match) => {
        const locMatch = match.match(/<loc>(.*?)<\/loc>/);
        if (!locMatch) return null;
        let url = locMatch[1];
        // CDATAã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†
        url = url.replace(/<!\[CDATA\[(.*?)\]\]>/g, '$1');
        return url.trim();
      }).filter((url): url is string => url !== null && url.length > 0);
      
      return urls;
    }
  } catch (error) {
    console.error('Error fetching sitemap:', error);
    return [];
  }
}

// BaseURLã‹ã‚‰URLãƒªã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®è‡ªå‹•æ¤œå‡ºã‚’è©¦è¡Œï¼‰
async function getUrlsFromBaseUrl(baseUrl: string): Promise<{
  urls: string[];
  detectedSitemapUrl?: string;
  detectionMethod?: string;
}> {
  const commonSitemapPaths = [
    '/sitemap.xml',
    '/sitemap_index.xml',
    '/sitemap-index.xml',
    '/sitemap1.xml',
    '/sitemap.txt',
  ];
  
  const normalizedBaseUrl = baseUrl.replace(/\/$/, '');
  const attemptedPaths: string[] = [];
  
  for (const path of commonSitemapPaths) {
    try {
      const sitemapUrl = `${normalizedBaseUrl}${path}`;
      attemptedPaths.push(sitemapUrl);
      const response = await fetch(sitemapUrl, { method: 'HEAD' });
      
      if (response.ok) {
        const contentType = response.headers.get('content-type');
        if (contentType && contentType.includes('xml')) {
          const urls = await getUrlsFromSitemap(sitemapUrl);
          if (urls.length > 0) {
            return {
              urls,
              detectedSitemapUrl: sitemapUrl,
              detectionMethod: `è‡ªå‹•æ¤œå‡ºï¼ˆ${path}ï¼‰`,
            };
          }
        }
      }
    } catch (error) {
      continue;
    }
  }
  
  // robots.txtã‚’ç¢ºèª
  try {
    const robotsUrl = `${normalizedBaseUrl}/robots.txt`;
    const robotsResponse = await fetch(robotsUrl);
    
    if (robotsResponse.ok) {
      const robotsText = await robotsResponse.text();
      const sitemapMatches = robotsText.match(/Sitemap:\s*(.+)/gi);
      
      if (sitemapMatches) {
        for (const match of sitemapMatches) {
          const sitemapUrl = match.replace(/Sitemap:\s*/i, '').trim();
          const urls = await getUrlsFromSitemap(sitemapUrl);
          if (urls.length > 0) {
            return {
              urls,
              detectedSitemapUrl: sitemapUrl,
              detectionMethod: 'è‡ªå‹•æ¤œå‡ºï¼ˆrobots.txtï¼‰',
            };
          }
        }
      }
    }
  } catch (error) {
    // ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
  }
  
  return {
    urls: [baseUrl],
    detectionMethod: 'ã‚µã‚¤ãƒˆãƒãƒƒãƒ—æœªæ¤œå‡ºï¼ˆãƒ™ãƒ¼ã‚¹URLã®ã¿ï¼‰',
  };
}

// URLãƒªã‚¹ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
async function extractDataFromUrls(
  urls: string[],
  onProgress?: (processed: number, total: number) => Promise<void> | void,
): Promise<Document[]> {
  console.log('extracting data from urls...');
  const documents: Document[] = [];
  const total = urls.length;
  let processed = 0;
  for (const url of urls) {
    try {
      const loader = new CustomWebLoader(url);
      const docs = await loader.load();
      documents.push(...docs);
    } catch (error) {
      console.error(`Error while extracting data from ${url}:`, error);
    }
    processed += 1;
    if (onProgress) {
      await onProgress(processed, total);
    }
  }
  console.log(`data extracted from ${documents.length} documents`);
  return documents;
}

// ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
async function splitDocsIntoChunks(docs: Document[]): Promise<Document[]> {
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 200,
  });
  return await textSplitter.splitDocuments(docs);
}

// ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¦‚ç®—è¨ˆç®—
function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4);
}

// åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã¦Supabaseã«ä¿å­˜ï¼ˆsite_idä»˜ãï¼‰
async function embedDocumentsWithSiteId(
  siteId: string,
  docs: Document[],
  embeddings: OpenAIEmbeddings,
  onProgress?: (processed: number, total: number) => void,
): Promise<number> {
  console.log(`[Worker] Creating embeddings for ${docs.length} documents...`);
  
  const uniqueId = `site_${siteId}_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  const docsWithMarker = docs.map((doc) => ({
    ...doc,
    metadata: {
      ...doc.metadata,
      _training_marker: uniqueId,
    },
  }));

  // SupabaseVectorStoreã§ä¿å­˜
  await SupabaseVectorStore.fromDocuments(docsWithMarker, embeddings, {
    client: supabase,
    tableName: 'documents',
  });

  // site_idã‚’è¨­å®š
  const { data: updatedDocs, error: updateError } = await supabase
    .from('documents')
    .update({ site_id: siteId })
    .eq('metadata->>_training_marker', uniqueId)
    .is('site_id', null)
    .select('id, metadata');

  if (updateError) {
    console.error('[Worker] Error updating site_id:', updateError);
    throw updateError;
  }

  // _training_markerã‚’å‰Šé™¤
  if (updatedDocs && updatedDocs.length > 0) {
    const updatePromises = updatedDocs.map(async (doc) => {
      if (doc.metadata && typeof doc.metadata === 'object') {
        const { _training_marker, ...cleanMetadata } = doc.metadata as any;
        return supabase
          .from('documents')
          .update({ metadata: cleanMetadata })
          .eq('id', doc.id);
      }
      return Promise.resolve();
    });
    await Promise.all(updatePromises);
  }

  const totalTokens = docs.reduce((sum, doc) => sum + estimateTokens(doc.pageContent), 0);
  return totalTokens;
}

// ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½œæˆ
const worker = new Worker<TrainingJobData>(
  'training-jobs',
  async (job: Job<TrainingJobData>) => {
    const {
      site_id,
      baseUrl,
      sitemapUrl,
      urlList,
      ownerUserId,
      requestedBy,
      forceRetrain = false,
    } = job.data;

    console.log(`[Worker] Processing job ${job.id} for site ${site_id}`);

    try {
      // 1. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ 'running' ã«æ›´æ–°
      await supabase
        .from('training_jobs')
        .update({
          status: 'running',
          started_at: new Date().toISOString(),
        })
        .eq('id', job.id);

      // 2. ã‚µã‚¤ãƒˆæƒ…å ±ã‚’å–å¾—
      const { data: site, error: siteError } = await supabase
        .from('sites')
        .select('*')
        .eq('id', site_id)
        .single();

      if (siteError || !site) {
        throw new Error(`Site not found: ${site_id}`);
      }

      // 3. å†å­¦ç¿’ã®å ´åˆã€æ—¢å­˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤
      if (forceRetrain) {
        console.log(`[Worker] Deleting existing documents for site_id: ${site_id}`);
        await supabase
          .from('documents')
          .delete()
          .eq('site_id', site_id);
      }

      // 4. URLãƒªã‚¹ãƒˆã‚’å–å¾—
      let urls: string[] = [];
      let detectedSitemapUrl: string | undefined;
      let detectionMethod: string | undefined;
      
      if (urlList && urlList.length > 0) {
        urls = urlList;
        detectionMethod = `URLãƒªã‚¹ãƒˆï¼ˆ${urls.length}ä»¶ï¼‰`;
      } else if (sitemapUrl) {
        urls = await getUrlsFromSitemap(sitemapUrl);
        detectionMethod = 'æ‰‹å‹•æŒ‡å®šï¼ˆã‚µã‚¤ãƒˆãƒãƒƒãƒ—ï¼‰';
      } else {
        const result = await getUrlsFromBaseUrl(baseUrl);
        urls = result.urls;
        detectedSitemapUrl = result.detectedSitemapUrl;
        detectionMethod = result.detectionMethod;
      }

      const originalUrlCount = urls.length;
      let wasTruncated = false;
      if (urls.length > MAX_TRAINING_PAGES) {
        urls = urls.slice(0, MAX_TRAINING_PAGES);
        wasTruncated = true;
      }

      // 5. ã‚¸ãƒ§ãƒ–ã®total_pagesã¨metadataã‚’æ›´æ–°
      await supabase
        .from('training_jobs')
        .update({ 
          total_pages: urls.length,
          metadata: {
            detected_sitemap_url: detectedSitemapUrl || sitemapUrl || null,
            detection_method: detectionMethod || 'ä¸æ˜',
            url_count: urls.length,
            original_url_count: originalUrlCount,
            page_limit: {
              max_pages: MAX_TRAINING_PAGES,
              truncated: wasTruncated,
            },
            urls: urls,
          },
        })
        .eq('id', job.id);

      // 6. ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
      const rawDocs = await extractDataFromUrls(urls, async (processed, total) => {
        await supabase
          .from('training_jobs')
          .update({ processed_pages: processed })
          .eq('id', job.id);
        
        // é€²æ—ã‚’æ›´æ–°
        await job.updateProgress({
          processed,
          total,
        });
      });
      
      // 7. ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
      const docs = await splitDocsIntoChunks(rawDocs);

      // 8. åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã¨ä¿å­˜
      const embeddings = new OpenAIEmbeddings({
        model: 'text-embedding-3-small',
        dimensions: 512,
      });

      const embeddingTokens = await embedDocumentsWithSiteId(site_id, docs, embeddings);

      // 9. ã‚³ã‚¹ãƒˆè¨ˆç®—
      const estimatedCostUsd = (embeddingTokens / 1_000_000) * 0.02;

      // 10. å®Œäº†å‡¦ç†
      await supabase
        .from('sites')
        .update({ 
          status: 'ready',
          last_trained_at: new Date().toISOString(),
        })
        .eq('id', site_id);

      await supabase
        .from('training_jobs')
        .update({
          status: 'completed',
          finished_at: new Date().toISOString(),
          processed_pages: urls.length,
          estimated_cost_usd: estimatedCostUsd,
        })
        .eq('id', job.id);

      // 11. usage_logsã«è¨˜éŒ²
      try {
        await supabase.from('usage_logs').insert({
          user_id: ownerUserId,
          site_id: site_id,
          action: 'training',
          model_name: 'text-embedding-3-small',
          tokens_consumed: embeddingTokens,
          cost_usd: estimatedCostUsd,
          metadata: {
            document_count: docs.length,
            url_count: urls.length,
            job_id: job.id,
            requested_by: requestedBy || ownerUserId,
          },
        });
      } catch (logError) {
        console.error('[Worker] Failed to log usage:', logError);
      }

      console.log(`[Worker] Job ${job.id} completed successfully`);
      return {
        success: true,
        processedPages: urls.length,
        totalPages: urls.length,
      };
    } catch (error: any) {
      console.error(`[Worker] Job ${job.id} failed:`, error);

      // ã‚¨ãƒ©ãƒ¼å‡¦ç†
      await supabase
        .from('training_jobs')
        .update({
          status: 'failed',
          finished_at: new Date().toISOString(),
          error_message: error.message || 'Unknown error',
          attempt: job.attemptsMade + 1,
        })
        .eq('id', job.id);

      await supabase
        .from('sites')
        .update({ status: 'error' })
        .eq('id', site_id);

      throw error;
    }
  },
  {
    connection,
    concurrency: 3,
    limiter: {
      max: 5,
      duration: 1000,
    },
  }
);

// ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
worker.on('completed', (job) => {
  console.log(`âœ… Job ${job.id} completed`);
});

worker.on('failed', (job, err) => {
  console.error(`âŒ Job ${job?.id} failed:`, err);
});

worker.on('error', (err) => {
  console.error('Worker error:', err);
});

// ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
process.on('SIGTERM', async () => {
  console.log('SIGTERM received, closing worker...');
  await worker.close();
  await connection.quit();
  process.exit(0);
});

process.on('SIGINT', async () => {
  console.log('SIGINT received, closing worker...');
  await worker.close();
  await connection.quit();
  process.exit(0);
});

console.log('ğŸš€ Training worker started');
console.log(`Redis: ${process.env.REDIS_HOST || 'localhost'}:${process.env.REDIS_PORT || '6379'}`);
